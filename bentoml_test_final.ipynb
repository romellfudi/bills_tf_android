{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bentoml-test-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romellfudi/bills_tf_android/blob/master/bentoml_test_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crYOucae98hl",
        "outputId": "704310fc-112e-4ccf-b51e-b043916b964f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%load_ext watermark\n",
        "%watermark -a \"Romell D.Z.\" -u -d -p tensorflow,numpy,PIL,keras"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The watermark extension is already loaded. To reload it, use:\n",
            "  %reload_ext watermark\n",
            "Romell D.Z. \n",
            "last updated: 2020-10-02 \n",
            "\n",
            "tensorflow 2.3.0\n",
            "numpy 1.18.5\n",
            "PIL 7.0.0\n",
            "keras 2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg4hDK_BwPAc"
      },
      "source": [
        "%%capture\n",
        "!pip install -q bentoml \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvrt61O1q5Lw"
      },
      "source": [
        "%%capture\n",
        "!rm -rf bills_photos/ __MACOSX/\n",
        "!curl -vLJO -H 'Accept: application/octet-stream' https://api.github.com/repos/romellfudi/bills_tf_android/releases/assets/26118074  -u \"contactboosttag:254aa92f4c88b57bdbc42070fbd0c66e58d00121\" \n",
        "!curl -vLJO -H 'Accept: application/octet-stream' https://api.github.com/repos/romellfudi/bills_tf_android/releases/assets/26360766  -u \"contactboosttag:254aa92f4c88b57bdbc42070fbd0c66e58d00121\" \n",
        "!unzip bills_photos.zip\n",
        "!find . -name '*.xml' -delete"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8OuPDSL_gAO",
        "cellView": "form",
        "outputId": "cfde3fb5-04b0-47f8-c2c6-c25413b0834d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#@title Slipt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "os.chdir('bills_photos')\n",
        "if os.path.isdir('train/10') is False:\n",
        "    os.makedirs('train/10')\n",
        "    os.makedirs('train/20')\n",
        "    os.makedirs('train/50')\n",
        "    os.makedirs('train/100')\n",
        "    os.makedirs('valid/10')\n",
        "    os.makedirs('valid/20')\n",
        "    os.makedirs('valid/50')\n",
        "    os.makedirs('valid/100')\n",
        "    os.makedirs('test/10')\n",
        "    os.makedirs('test/20')\n",
        "    os.makedirs('test/50')\n",
        "    os.makedirs('test/100')\n",
        "\n",
        "    for i in random.sample(glob.glob('Billetes 10/*.JPG'), 120):\n",
        "        shutil.move(i, 'train/10')      \n",
        "    for i in random.sample(glob.glob('Billetes 20/*.JPG'), 120):\n",
        "        shutil.move(i, 'train/20')   \n",
        "    for i in random.sample(glob.glob('Billetes 50/*.JPG'), 120):\n",
        "        shutil.move(i, 'train/50')   \n",
        "    for i in random.sample(glob.glob('Billetes 100/*.JPG'), 120):\n",
        "        shutil.move(i, 'train/100')\n",
        "    for i in random.sample(glob.glob('Billetes 10/*.JPG'), 60):\n",
        "        shutil.move(i, 'valid/10')   \n",
        "    for i in random.sample(glob.glob('Billetes 20/*.JPG'), 60):\n",
        "        shutil.move(i, 'valid/20')   \n",
        "    for i in random.sample(glob.glob('Billetes 50/*.JPG'), 60):\n",
        "        shutil.move(i, 'valid/50')        \n",
        "    for i in random.sample(glob.glob('Billetes 100/*.JPG'), 60):\n",
        "        shutil.move(i, 'valid/100')\n",
        "    for i in random.sample(glob.glob('Billetes 10/*.JPG'), 20):\n",
        "        shutil.move(i, 'test/10')  \n",
        "    for i in random.sample(glob.glob('Billetes 20/*.JPG'), 20):\n",
        "        shutil.move(i, 'test/20')  \n",
        "    for i in random.sample(glob.glob('Billetes 50/*.JPG'), 20):\n",
        "        shutil.move(i, 'test/50')      \n",
        "    for i in random.sample(glob.glob('Billetes 100/*.JPG'), 20):\n",
        "        shutil.move(i, 'test/100')\n",
        "\n",
        "os.chdir('../')\n",
        "\n",
        "train_path = 'bills_photos/train'\n",
        "valid_path = 'bills_photos/valid'\n",
        "test_path = 'bills_photos/test'\n",
        "\n",
        "train_batches = ImageDataGenerator(rotation_range=90) \\\n",
        "    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['10', '20', '50', '100'], batch_size=10)\n",
        "valid_batches = ImageDataGenerator(rotation_range=90) \\\n",
        "    .flow_from_directory(directory=valid_path, target_size=(224,224), classes=['10', '20', '50', '100'], batch_size=10)\n",
        "test_batches = ImageDataGenerator(rotation_range=90) \\\n",
        "    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['10', '20', '50', '100'], batch_size=10, shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 480 images belonging to 4 classes.\n",
            "Found 240 images belonging to 4 classes.\n",
            "Found 80 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqoJbeUdALwE",
        "outputId": "c4ca6ab6-4b9b-43d8-a5e0-6909cb929ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p = 'bills_photos/train/20/*.JPG'\n",
        "a=!ls {p}\n",
        "p = a[2].split()[0]\n",
        "!du -sh {p}"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20K\tbills_photos/train/20/P9030006.JPG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR3WTA_JrV06",
        "outputId": "6f7e444d-f917-449e-c567-ef5a1bb6d406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "model = load_model('image_generation_model.h5')\n",
        "inputs = [] \n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "img = Image.open(p) #TODO\n",
        "def preprocess_image(image, target_size):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    image = image.resize(target_size)\n",
        "    return np.expand_dims(img_to_array(image), axis=0)\n",
        "img = preprocess_image(img, target_size=(224, 224)) \n",
        "inputs.append(img)\n",
        "inputs.append(img)\n",
        "stack = np.vstack(inputs) \n",
        "model.predict(stack).tolist()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdbdc20f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.1062706666416433e-15, 1.0, 8.008348845178759e-11, 4.712596974573756e-16],\n",
              " [2.1062706666416433e-15, 1.0, 8.008348845178759e-11, 4.712596974573756e-16]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRPIlbCoU1fN"
      },
      "source": [
        "class Bills(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Bills, self).__init__()\n",
        "        self.cnn = tf.keras. Sequential([\n",
        "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n",
        "            tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "            tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
        "            tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(units=4, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    @staticmethod\n",
        "    def image_bytes2tensor(inputs):\n",
        "        with tf.device(\"cpu:0\"):  # map_fn has issues on GPU https://github.com/tensorflow/tensorflow/issues/28007\n",
        "            inputs = tf.map_fn(lambda i: tf.io.decode_png(i, channels=3), inputs, dtype=tf.uint8)\n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "        inputs = (255.0 - inputs) / 255.0\n",
        "        inputs = tf.reshape(inputs, [-1, 224, 224,3])\n",
        "        return inputs\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,), dtype=tf.string)])\n",
        "    def predict_image(self, inputs):\n",
        "        inputs = self.image_bytes2tensor(inputs)\n",
        "        return self(inputs)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.cnn(inputs)\n",
        "\n",
        "model_ = Bills()"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seIk4Tup5gFj",
        "outputId": "f1e96b27-7513-4f88-85ae-deb690b06e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model_.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_.fit(x=train_batches,\n",
        "    steps_per_epoch=len(train_batches),\n",
        "    validation_data=valid_batches,\n",
        "    validation_steps=len(valid_batches),\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "48/48 [==============================] - 47s 971ms/step - loss: 54.4785 - accuracy: 0.3792 - val_loss: 6.1647 - val_accuracy: 0.5292\n",
            "Epoch 2/3\n",
            "48/48 [==============================] - 45s 941ms/step - loss: 4.9134 - accuracy: 0.6187 - val_loss: 3.0930 - val_accuracy: 0.6542\n",
            "Epoch 3/3\n",
            "48/48 [==============================] - 45s 941ms/step - loss: 4.1381 - accuracy: 0.6292 - val_loss: 2.1595 - val_accuracy: 0.7833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdbdc36c828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fweBGykWBHCh",
        "outputId": "5e2c097d-3214-4ff9-f625-58e06eec92a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile tensorflow_bills.py\n",
        "from typing import List\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from bentoml import BentoService, api, artifacts, env\n",
        "from bentoml.frameworks.tensorflow import TensorflowSavedModelArtifact\n",
        "from bentoml.adapters import TfTensorInput, ImageInput, JsonInput\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "BILLS_CLASSES = ['B10','B20','B50','B100']\n",
        "\n",
        "@env(pip_dependencies=['tensorflow', 'numpy', 'pillow'])\n",
        "@artifacts([TensorflowSavedModelArtifact('model')])\n",
        "class KerasBillsService(BentoService):\n",
        "    @api(input=TfTensorInput(), batch=True)\n",
        "    def predict(self, tensors):\n",
        "        inputs = []\n",
        "        for tensor in tensors.numpy():\n",
        "            image = Image.open(io.BytesIO(tensor))\n",
        "            if image.mode != \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            image = image.resize((224,224 ))\n",
        "            image = np.expand_dims(img_to_array(image), axis=0)\n",
        "            inputs.append(image)\n",
        "        inputs_data = np.vstack(inputs)\n",
        "        prediction = self.artifacts.model(inputs_data)\n",
        "        return [(BILLS_CLASSES[np.argmax(i)],i.numpy()) for i in prediction]"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tensorflow_bills.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3F6cFkt2mC",
        "outputId": "d1de62ee-8100-4ddb-b0fa-f743d44553a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from tensorflow_bills import KerasBillsService\n",
        "image_generation = KerasBillsService()\n",
        "image_generation.pack('model', model) # From my repository\n",
        "# image_generation.pack('model', model_) # it was built here\n",
        "saved_path = image_generation.save()"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-02 01:24:24,018] WARNING - pip package requirement tensorflow already exist\n",
            "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-ux4qri5t/KerasBillsService/artifacts/model_saved_model/assets\n",
            "[2020-10-02 01:24:25,437] INFO - BentoService bundle 'KerasBillsService:20201002012424_CD4A75' saved to: /root/bentoml/repository/KerasBillsService/20201002012424_CD4A75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhqWEAnnzz3_"
      },
      "source": [
        "!bentoml run KerasBillsService:latest predict --input='{\"instances\": [{\"b64\": \"iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAYAAAAaLWrhAAAAAXNSR0IArs4c6QAAAIRlWElmTU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUAAAABAAAASgEbAAUAAAABAAAAUgEoAAMAAAABAAIAAIdpAAQAAAABAAAAWgAAAAAAAABIAAAAAQAAAEgAAAABAAOgAQADAAAAAQABAACgAgAEAAAAAQAAAOCgAwAEAAAAAQAAAOAAAAAAUWNJNwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAVlpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDUuNC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iPgogICAgICAgICA8dGlmZjpPcmllbnRhdGlvbj4xPC90aWZmOk9yaWVudGF0aW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KTMInWQAAA4NJREFUeAHt0AENAAAAwqD3T20PBxEoDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDPwMDBENAAFep7MsAAAAAElFTkSuQmCC\"},{\"b64\": \"iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAYAAAAaLWrhAAAAAXNSR0IArs4c6QAAAIRlWElmTU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUAAAABAAAASgEbAAUAAAABAAAAUgEoAAMAAAABAAIAAIdpAAQAAAABAAAAWgAAAAAAAABIAAAAAQAAAEgAAAABAAOgAQADAAAAAQABAACgAgAEAAAAAQAAAOCgAwAEAAAAAQAAAOAAAAAAUWNJNwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAVlpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDUuNC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iPgogICAgICAgICA8dGlmZjpPcmllbnRhdGlvbj4xPC90aWZmOk9yaWVudGF0aW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KTMInWQAAA4NJREFUeAHt0AENAAAAwqD3T20PBxEoDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDBgwYMCAAQMGDPwMDBENAAFep7MsAAAAAElFTkSuQmCC\"} ]}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeR8fI9AASjK"
      },
      "source": [
        "# response \n",
        "# [[\"B20\", [0.24972102046012878, 0.251900315284729, 0.2476738840341568, 0.250704824924469]], [\"B20\", [0.24972102046012878, 0.251900315284729, 0.2476738840341568, 0.250704824924469]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtx1gVLUSJb"
      },
      "source": [
        "import base64,json\n",
        "with open(p, \"rb\") as f:\n",
        "    img_bytes = f.read()\n",
        "img_b64 = base64.b64encode(img_bytes).decode()\n",
        "data = json.dumps({\"instances\": [{\"b64\": img_b64}]})"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuqGno9QYFwd"
      },
      "source": [
        "!bentoml run KerasBillsService:latest predict --input='{data}' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9upHT8z19Li"
      },
      "source": [
        "p = 'bills_photos/train/100/*.JPG'\n",
        "p = a[4].split()[0] \n",
        "with open(p, \"rb\") as f:\n",
        "    img_bytes = f.read()\n",
        "img_b64 = base64.b64encode(img_bytes).decode()\n",
        "data = json.dumps({\"instances\": [{\"b64\": img_b64}]})\n",
        "!bentoml run KerasBillsService:latest predict --input='{data}' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybqpTbcX7DgN"
      },
      "source": [
        "!bentoml serve KerasBillsService:latest --run-with-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw1-CnEx6xM3"
      },
      "source": [
        "curl -i \\\n",
        "    --request POST \\\n",
        "    --header \"Content-Type: application/json\" \\\n",
        "    --data {data} \\\n",
        "    http://b58c50af9d79.ngrok.io/predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRDujE4O8SiJ"
      },
      "source": [
        "curl -i \\\n",
        "    --request POST \\\n",
        "    --header \"Content-Type: application/json\" \\\n",
        "    --data $a \\\n",
        "    http://b58c50af9d79.ngrok.io/predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UnmLU61Asei"
      },
      "source": [
        "# [[\"B20\", [4.5302891749088303e-07, 0.9496379494667053, 0.050349801778793335, 1.172510292235529e-05]]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}